{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TE Academy Reputation-Weighted Voting \n",
    "\n",
    "## Voting Mechanism Experiment \n",
    "\n",
    "## Experiment #1-B: Feasibility of Default Weights for June 19 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Overview\n",
    "\n",
    "### Key Question\n",
    "\n",
    "Does the initial manually tuned setting of weights satisfy the key requirements laid out for it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, Part 1\n",
    "### Voting Mechanism Choice\n",
    "\n",
    "We need to design on a mechanism that we want to use, and import it. \n",
    "The mechanism needs to be in the `mechanisms` module. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1, Step 1:  Explanation of Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We load the `PercentageAllocationWeightedPlurality` mechanism.\n",
    "\n",
    "This mechanism \n",
    "1. uses weights (point totals) defined for each user\n",
    "2. has each user assign percentage value to each candidate, such that values total to 1.0.\n",
    "3. allocates to each candidate the proportion of the user's weight indicated by their percentage.\n",
    "4. winning candidate is the one with the most weight. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Example:**\n",
    "1. Voter 1 is assigned a weight of 8 points. \n",
    "2. Voter 1 assigns the following percentages:\n",
    "```python\n",
    "ballot = {\"candidate_A\": 0.6,\n",
    "          \"candidate_B\": 0.3,\n",
    "          \"candidate_C\": 0.1}\n",
    "```\n",
    "3. After processing this ballot, \n",
    "* `candidate_A`'s vote total will increase by 0.6 * 8 = 4.8 votes\n",
    "* `candidate_B`'s vote total will increase by 0.3 * 8 = 2.4 votes\n",
    "* `candidate_C`'s vote total will increase by 0.1 * 8 = 0.8 votes\n",
    "\n",
    "If Voter 1 was the only voter, `candidate_A` would win. If there are more Voters, we will continue processing Vote results in this way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1, Step 2: Actual Code Import Statements\n",
    "\n",
    "We import several things that the code needs to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')  # Add this line to include the directory above\n",
    "\n",
    "# Import Data Processing Utilities\n",
    "from utils.processing import (remove_blocklist_addresses,\n",
    "                              validate_dataframe_june_19,\n",
    "                              preprocess_data,\n",
    "                              add_tef_graduate_column,\n",
    "                              calculate_nft_weighted_sum,\n",
    "                              create_dict_for_equal_cweights)\n",
    "\n",
    "# Import Actual Voting Mechanism\n",
    "from mechanisms.percentage_allocation_weighted_plurality import PercentageAllocationWeightedPlurality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Part 2: Importing the User NFT Data\n",
    "\n",
    "In the cell below, we read data from a .csv file about which users have which NFTs. \n",
    "Creating the .csv file will need to be done elsewhere.\n",
    "\n",
    "**TODO:** Put an up-to-date GraphQL pull elsewhere, so data can be updated from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"../data/2024-06-19_nft_balances.csv\"\n",
    "file_exists = os.path.exists(file_name)\n",
    "file_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "user_data = pd.read_csv(file_name)\n",
    "\n",
    "# Set the index of the DataFrame to be Id\n",
    "user_data.set_index('Id', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check to see that everything is valid, clean if not. \n",
    "user_data = validate_dataframe_june_19(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCKLIST_ADDRESSES = [\"0xa55078f87ceDa4aC72380C639229014acD3D1F75\",\n",
    "\"0xc6837f9d06D95Fa90CF91A6Dd6bB8cb51bfcfc59\",\n",
    "\"0x4CF57d42B8aB8D7Bfa9Be1cdC35Ed84429cD2168\"]\n",
    "\n",
    "user_data = remove_blocklist_addresses(df = user_data, \n",
    "                                       blocklist = BLOCKLIST_ADDRESSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = preprocess_data(df = user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the user_data DataFrame to a nested dictionary\n",
    "user_data_dict = user_data.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make sure that the data has been sufficiently cleaned, and has the expected properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add up all of the entries in a row and show the ten largest\n",
    "row_sums = user_data.sum(axis=1)\n",
    "top_ten_sums = row_sums.nlargest(10)\n",
    "print(top_ten_sums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup **Part 3**: Defining NFT Information\n",
    "\n",
    "### Part 3, Step 1: Defining NFT Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Option (a):** Defining NFT Weights Manually.\n",
    "\n",
    "Each NFT weight can be set as part of a dictionary. If you choose to do this, please note that you will need to delete (or comment out) the parts below that read NFT weights as a .csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_weights_dict = {\n",
    "    \"tokenId 1\": 7,   # Token ID 1\n",
    "    \"tokenId 2\": 0,   # Token ID 2\n",
    "    \"tokenId 3\": 7,   # Token ID 3\n",
    "    \"tokenId 4\": 0,   # Token ID 4\n",
    "    \"tokenId 5\": 7,   # Token ID 5\n",
    "    \"tokenId 6\": 0,   # Token ID 6\n",
    "    \"tokenId 7\": 7,   # Token ID 7\n",
    "    \"tokenId 8\": 0,   # Token ID 8\n",
    "    \"tokenId 9\": 7,   # Token ID 9\n",
    "    \"tokenId 10\": 0,  # Token ID 10\n",
    "    \"tokenId 11\": 20, # Token ID 11\n",
    "    \"tokenId 12\": 10, # Token ID 12\n",
    "    \"tokenId 13\": 10, # Token ID 13\n",
    "    \"tokenId 14\": 10, # Token ID 14\n",
    "    \"tokenId 15\": 16, # Token ID 15\n",
    "    \"tokenId 16\": 1,  # Token ID 16\n",
    "    \"tokenId 17\": 16, # Token ID 17\n",
    "    \"tokenId 18\": 5,  # Token ID 18\n",
    "    \"tokenId 19\": 1,  # Token ID 19\n",
    "    \"tokenId 20\": 3,  # Token ID 20\n",
    "    \"tokenId 21\": 3,  # Token ID 21\n",
    "    \"tokenId 22\": 1,  # Token ID 22\n",
    "    \"tokenId 23\": 10, # Token ID 23\n",
    "    \"tokenId 24\": 10, # Token ID 24\n",
    "    \"tokenId 25\": 10, # Token ID 25\n",
    "    \"tokenId 26\": 10, # Token ID 26\n",
    "    \"tokenId 27\": 10, # Token ID 27\n",
    "    \"tokenId 28\": 10, # Token ID 28\n",
    "    \"tokenId 29\": 10, # Token ID 29\n",
    "    \"tokenId 30\": 10, # Token ID 30\n",
    "    \"tokenId 31\": 15, # Token ID 31\n",
    "    \"tokenId 32\": 15, # Token ID 32\n",
    "    \"tokenId 33\": 15, # Token ID 33\n",
    "    \"tokenId 34\": 15, # Token ID 34\n",
    "    \"tokenId 35\": 15, # Token ID 35\n",
    "    \"tokenId 36\": 15, # Token ID 36\n",
    "    \"tokenId 37\": 15, # Token ID 37\n",
    "    \"tokenId 38\": 15, # Token ID 38\n",
    "    \"tokenId 39\": 18, # Token ID 39\n",
    "    \"tokenId 40\": 1,  # Token ID 40\n",
    "    \"tokenId 41\": 4, # Token ID 41\n",
    "    \"tokenId 42\": 18, # Token ID 42\n",
    "    \"tokenId 43\": 16,  # Token ID 43\n",
    "    \"tokenId 44\": 1,  # Token ID 44,\n",
    "    \"tokenId 45\": 10, # Token ID 45,\n",
    "    \"tef_graduate\": 0, #will be set in a moment\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, Part 3\n",
    "### Step 1, Option (b): Reading NFT Weights from a .csv File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to load the above weights as a default .csv file, but we will skip that for now. \n",
    "\n",
    "This remains a **TODO**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, Part 3: Defining NFT Information\n",
    "\n",
    "### Step 2: Giving Additional NFT Group Information\n",
    "\n",
    "We also divide NFT information into which Tokens belong to ```experts```, and which belong to ```students```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_token_Ids = [f\"tokenId {num}\" for num in [11, 15, 17, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45]]\n",
    "graduates_tokenIds = [f\"tokenId {num}\" for num in  [1,2,3,4,5,6,7,8,9,10]]\n",
    "student_tokenIds = [f\"tokenId {num}\" for num in [12, 13, 14, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, Part 4: Setting Additional Information\n",
    "\n",
    "There are certain weighting options where it may be interesting to either assign or analyze voter information, based on additional knowledge about the NFTs. \n",
    "\n",
    "This information may include: \n",
    "1. How many of each NFT there are.\n",
    "2. How each NFT is characterized: \"student\", \"graduate\", or \"expert\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Remove zeros from NFTs prior to analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4, Step 1: Counting The Total Number of Each NFT\n",
    "\n",
    "We count and visualize the number of each NFT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums = user_data.sum(axis=0)\n",
    "\n",
    "# Create a sorted bar chart\n",
    "sorted_sums = column_sums.sort_values(ascending=False)\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Define colors for each group\n",
    "colors = []\n",
    "for token_id in sorted_sums.index.str.replace('tokenId ', ''):\n",
    "    if int(token_id) in expert_token_Ids:\n",
    "        colors.append('blue')  # Color for experts\n",
    "    elif int(token_id) in graduates_tokenIds:\n",
    "        colors.append('green')  # Color for graduates\n",
    "    elif int(token_id) in student_tokenIds:\n",
    "        colors.append('red')  # Color for students\n",
    "    else:\n",
    "        colors.append('gray')  # Color for undefined groups\n",
    "\n",
    "plt.bar(sorted_sums.index.str.replace('tokenId ', ''), sorted_sums.values, color=colors)\n",
    "plt.xlabel('Token ID')\n",
    "plt.ylabel('Sum')\n",
    "plt.title('Count for each NFT')\n",
    "\n",
    "# Calculate mean and median\n",
    "mean_value = sorted_sums.mean()\n",
    "median_value = sorted_sums.median()\n",
    "\n",
    "# Add vertical lines for mean and median\n",
    "plt.axvline(x=mean_value, color='magenta', linestyle='--', label=f'Mean: {mean_value:.2f}')\n",
    "plt.axvline(x=median_value, color='cyan', linestyle='-', label=f'Median: {median_value:.2f}')\n",
    "\n",
    "# Add legend for groups and statistics\n",
    "# `bbox_to_anchor` is used to place the legend at a specific position. \n",
    "# The tuple (1, 1) places the legend at the upper right corner of the plot.\n",
    "group_legend = plt.legend([plt.Line2D([0], [0], color='blue'), plt.Line2D([0], [0], color='green'), plt.Line2D([0], [0], color='red')], \n",
    "                          ['Experts', 'Graduates', 'Students'], title=\"Groups\", loc='upper right', bbox_to_anchor=(1, 1))\n",
    "plt.gca().add_artist(group_legend)\n",
    "\n",
    "# Here, `bbox_to_anchor` is set to (1, 0.85), which places the legend slightly below the upper right corner.\n",
    "plt.legend([plt.Line2D([0], [0], color='magenta', linestyle='--'), plt.Line2D([0], [0], color='cyan', linestyle='-')], \n",
    "           [f'Mean: {mean_value:.2f}', f'Median: {median_value:.2f}'], title=\"Statistics\", loc='upper right', bbox_to_anchor=(1, 0.8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `user_data` is the DataFrame you want to sum the rows of for each user\n",
    "user_sums = user_data.sum(axis=1)\n",
    "\n",
    "# Create a sorted bar chart\n",
    "sorted_user_sums = user_sums.sort_values(ascending=False)\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Calculate the range for bins\n",
    "bin_range = (0, sorted_user_sums.max())\n",
    "bins = np.arange(bin_range[0], bin_range[1] + 2, 1)  # +2 to include the last bin edge\n",
    "\n",
    "# Plot histogram with heights as percentages\n",
    "histogram = plt.hist(sorted_user_sums, bins=bins, color='blue', edgecolor='black', weights=np.ones_like(sorted_user_sums) / len(sorted_user_sums))\n",
    "plt.xlabel('NFTs held')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('How Many Users Hold Certain Numbers of NFTs?')\n",
    "\n",
    "# Calculate the percentage of users holding 1 NFT or less\n",
    "one_nft_or_less = np.sum(sorted_user_sums <= 1) / len(sorted_user_sums) * 100\n",
    "\n",
    "plt.gca().text(0.85, 0.9, f\"{one_nft_or_less:.2f}% have\\n  1 NFT or less \\n\", \n",
    "              transform=plt.gca().transAxes, fontsize=12, color='red', \n",
    "              horizontalalignment='center', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "# Calculate mean and median\n",
    "mean_value = sorted_user_sums.mean()\n",
    "median_value = sorted_user_sums.median()\n",
    "\n",
    "# Add horizontal lines for mean and median\n",
    "plt.axvline(x=mean_value, color='magenta', linestyle='--', label=f'Mean: {mean_value:.2f}')\n",
    "plt.axvline(x=median_value, color='cyan', linestyle='-', label=f'Median: {median_value:.2f}')\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1, 0.8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums_dict = column_sums.to_dict()\n",
    "weighted_sums_dict = {token_id: sum * default_weights_dict.get(token_id, 0) \n",
    "                      for token_id, sum \n",
    "                      in column_sums_dict.items() if sum * default_weights_dict.get(token_id, 0) > 0}\n",
    "weighted_sums_series = pd.Series(weighted_sums_dict)\n",
    "weighted_sums_series = weighted_sums_series.sort_values(ascending=False)\n",
    "plt.figure(figsize=(12, 10))  # Assuming a reasonable width of 12 inches\n",
    "# Define colors for each group\n",
    "colors = []\n",
    "for token_id in weighted_sums_series.index.str.replace('tokenId ', ''):\n",
    "    if int(token_id) in expert_token_Ids:\n",
    "        colors.append('blue')  # Color for experts\n",
    "    elif int(token_id) in graduates_tokenIds:\n",
    "        colors.append('green')  # Color for graduates\n",
    "    elif int(token_id) in student_tokenIds:\n",
    "        colors.append('red')  # Color for students\n",
    "    else:\n",
    "        colors.append('gray')  # Color for undefined groups\n",
    "\n",
    "# Calculate the weighted sum for each NFT\n",
    "plt.bar(weighted_sums_series.index.str.replace('tokenId ', ''), weighted_sums_series.values, color=colors)\n",
    "plt.xlabel('Token ID')\n",
    "plt.ylabel('Weighted Sum')\n",
    "plt.title('Weighted for each NFT')\n",
    "\n",
    "# Calculate mean and median for weighted sums\n",
    "mean_value = weighted_sums_series.mean()\n",
    "median_value = weighted_sums_series.median()\n",
    "\n",
    "# Add vertical lines for mean and median\n",
    "plt.axvline(x=mean_value, color='magenta', linestyle='--', label=f'Mean: {mean_value:.2f}')\n",
    "plt.axvline(x=median_value, color='cyan', linestyle='-', label=f'Median: {median_value:.2f}')\n",
    "\n",
    "# Add legend for groups and statistics\n",
    "# `bbox_to_anchor` is used to place the legend at a specific position. \n",
    "# The tuple (1, 1) places the legend at the upper right corner of the plot.\n",
    "group_legend = plt.legend([plt.Line2D([0], [0], color='blue'), plt.Line2D([0], [0], color='green'), plt.Line2D([0], [0], color='red')], \n",
    "                          ['Experts', 'Graduates', 'Students'], title=\"Groups\", loc='upper right', bbox_to_anchor=(1, 1))\n",
    "plt.gca().add_artist(group_legend)\n",
    "\n",
    "# Here, `bbox_to_anchor` is set to (1, 0.85), which places the legend slightly below the upper right corner.\n",
    "plt.legend([plt.Line2D([0], [0], color='magenta', linestyle='--'), plt.Line2D([0], [0], color='cyan', linestyle='-')], \n",
    "           [f'Mean: {mean_value:.2f}', f'Median: {median_value:.2f}'], title=\"Statistics\", loc='upper right', bbox_to_anchor=(1, 0.8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "top_ten_entries = weighted_sums_series.nlargest(10).to_frame(name='Weight')\n",
    "bottom_ten_entries = weighted_sums_series.nsmallest(10).sort_values(ascending=False).to_frame(name='Weight')\n",
    "\n",
    "# Add a new column to indicate the group\n",
    "top_ten_entries['Group'] = top_ten_entries.index.str.replace('tokenId ', '').map(lambda x: 'Expert' if int(x) in expert_token_Ids else 'Graduate' if int(x) in graduates_tokenIds else 'Student' if int(x) in student_tokenIds else 'Undefined')\n",
    "bottom_ten_entries['Group'] = bottom_ten_entries.index.str.replace('tokenId ', '').map(lambda x: 'Expert' if int(x) in expert_token_Ids else 'Graduate' if int(x) in graduates_tokenIds else 'Student' if int(x) in student_tokenIds else 'Undefined')\n",
    "\n",
    "html_table_top = top_ten_entries.to_html(index=True)\n",
    "html_table_bottom = bottom_ten_entries.to_html(index=True)\n",
    "display(HTML(\"<h2>Top 10 NFTs by Certificate Weights:</h2><table style='float:left'>{}</table></br><h2>Bottom 10 NFTs by Certificate Weight:</h2><table style='float:left'>{}</table>\".format(html_table_top, html_table_bottom)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total weight per group\n",
    "expert_weight = column_sums[sorted_sums.index.str.replace('tokenId ', '').map(lambda x: int(x) in expert_token_Ids)].sum()\n",
    "graduate_weight = column_sums[sorted_sums.index.str.replace('tokenId ', '').map(lambda x: int(x) in graduates_tokenIds)].sum()\n",
    "student_weight = column_sums[sorted_sums.index.str.replace('tokenId ', '').map(lambda x: int(x) in student_tokenIds)].sum()\n",
    "\n",
    "# Create a simple three-color bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Experts', 'Graduates', 'Students'], [expert_weight, graduate_weight, student_weight], color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Group')\n",
    "plt.ylabel('Total Weight')\n",
    "plt.title('Total Weight per NFT Group')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, Step 5: Setting Voter Weights Based On NFT Weights\n",
    "\n",
    "Each voter needs their **own individual weight/points** based on the given information of which NFTs voters hold, and how much each NFT should count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_weights_dict = {}\n",
    "\n",
    "for user, row in user_data.iterrows():\n",
    "    weighted_sum = sum(row[col] * default_weights_dict.get(col, 0) for col in row.index)\n",
    "    voter_weights_dict[user] = {\"weight\": weighted_sum}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_voter_weights = sorted(voter_weights_dict.items(), key=lambda x: x[1]['weight'], reverse=True)\n",
    "voter_names = [voter[0] for voter in sorted_voter_weights]\n",
    "voter_weights = [voter[1]['weight'] for voter in sorted_voter_weights]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(voter_names, voter_weights, color='skyblue')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Voter Weights')\n",
    "plt.xticks([])  # Suppress the xticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_voter_weights = sorted(voter_weights_dict.items(), key=lambda x: x[1]['weight'], reverse=True)\n",
    "voter_weights = [voter[1]['weight'] for voter in sorted_voter_weights]\n",
    "\n",
    "min_weight = min(voter_weights)\n",
    "max_weight = max(voter_weights)\n",
    "bin_width = 5\n",
    "bins = [min_weight + i * bin_width for i in range((max_weight - min_weight) // bin_width + 1)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(voter_weights, bins=bins, color='skyblue')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Voter Weights Histogram')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sorted_voter_weights = sorted(voter_weights_dict.items(), key=lambda x: x[1]['weight'], reverse=True)\n",
    "weight_counts = Counter(voter[1]['weight'] for voter in sorted_voter_weights)\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "display(HTML(\"<h2>Voter Weight Frequencies</h2>\"))\n",
    "display(HTML(\"<table><tr><th>Weight</th><th>Frequency</th></tr>\" + \n",
    "             \"\".join(f\"<tr><td>{weight}</td><td>{count}</td></tr>\" for weight, count in weight_counts.most_common()) + \n",
    "             \"</table>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sorted_voter_weights = sorted(voter_weights_dict.items(), key=lambda x: x[1]['weight'], reverse=True)\n",
    "weight_counts = sorted(Counter(voter[1]['weight'] for voter in sorted_voter_weights).items(), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "display(HTML(\"<h2>Voter Weight Frequencies</h2>\"))\n",
    "display(HTML(\"<table><tr><th>Weight</th><th>Frequency</th></tr>\" + \n",
    "             \"\".join(f\"<tr><td>{weight}</td><td>{count}</td></tr>\" for weight, count in weight_counts) + \n",
    "             \"</table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Scenario Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "These help create specific scenarios that can then be extended to examine what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_voter_turnout(voters, percentage: float):\n",
    "    num_voters_to_sample = int(len(voters) * percentage)\n",
    "    sampled_voters = random.sample(sorted(voters), num_voters_to_sample)\n",
    "    sampled_voters_dict = {voter: voters[voter] for voter in sampled_voters}\n",
    "    return sampled_voters_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_random_voter_turnout(voter_weights_dict, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add up all the voter weights in a voter weights dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_voter_weights(input_voter_weights_dict):\n",
    "    # Add all the weights in a given voter weights dictionary. \n",
    "    return sum(voter['weight'] for voter in input_voter_weights_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_voter_choices(voters, candidates):\n",
    "    \"\"\"\n",
    "    Generates a dictionary of voter choices, where each voter's choices are a random distribution over the candidates.\n",
    "    \n",
    "    Parameters:\n",
    "    voters (list): A list of voter IDs\n",
    "    candidates (list): A list of candidate IDs\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary where each key is a voter ID and the value is a dictionary of candidate IDs to random weights\n",
    "    \"\"\"\n",
    "    voter_choices = {voter_id: {candidate: random.random() for candidate in candidates} for voter_id in voters}\n",
    "    for voter_id in voter_choices:\n",
    "        total = sum(voter_choices[voter_id].values())\n",
    "        voter_choices[voter_id] = {candidate: value / total for candidate, value in voter_choices[voter_id].items()}\n",
    "    return voter_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unanimous_voter_choices(voters, chosen_candidate):\n",
    "    \"\"\"\n",
    "    Generates a dictionary of voter choices where every voter unanimously chooses the given candidate.\n",
    "    \n",
    "    Parameters:\n",
    "    voters (list): A list of voter IDs\n",
    "    chosen_candidate (str): The ID of the candidate that every voter chooses\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary where each key is a voter ID and the value is a dictionary with the chosen candidate as the key and 1.0 as the value\n",
    "    \"\"\"\n",
    "    voter_choices = {voter_id: {chosen_candidate: 1.0} for voter_id in voters}\n",
    "    return voter_choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAWP = PercentageAllocationWeightedPlurality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement 1: Vote Distortion\n",
    "\n",
    "It's formally provable that with the kind of weighted voting method we are discussing, a voter who expresses at least equivalent support for a candidate cannot cause that candidate to lose. \n",
    "\n",
    "**Proof (Sketch):** Suppose that a candidate currently wins with $p \\in [0,1]$ of the overall vote, and $W$ total weight voting. A new voter comes along with weight $w_v$, and allocates $p_v \\geq p$ to the previous winning candidate. \n",
    "\n",
    "After this new vote, the previously winning candidate now has:\n",
    "$$p_{\\text{new}} = \\displaystyle\\frac{p \\cdot W + p_v \\cdot w_v}{W + w_v} \\geq \\displaystyle\\frac{p(W + w_v)}{W + w_v} = p.$$ \n",
    "\n",
    "Since $p_{\\text{new}} \\geq p$, the previously winning candidate still wins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "If we wish, we could supplement the proof with a simulation test. \n",
    "\n",
    "We would run some number of rnadomly simulated elections, with \n",
    "* randomly selected turnout.\n",
    "* randomly selected vote.\n",
    "\n",
    "We choose a voter not  voting in the current election, we have them vote for the current winner, and run a new election. \n",
    "If their vote changes the result so the prior winner now loses, we have a problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vote_distortion(mechanism,\n",
    "                         voters, \n",
    "                         candidates, \n",
    "                         num_trials):\n",
    "    trial_num = 0\n",
    "    num_vote_distortions = 0\n",
    "    bad_examples = []\n",
    "\n",
    "    while trial_num <= num_trials:\n",
    "        randomly_chosen_num_voters = random.randint(1,len(voters) - 1)\n",
    "        original_election_voter_IDs = random.sample(list(voters.keys()),\n",
    "                                                randomly_chosen_num_voters)\n",
    "        original_election_voters = {key: voters[key]\n",
    "                                     for key \n",
    "                                     in original_election_voter_IDs}\n",
    "        original_voter_choices = generate_random_voter_choices(voters = original_election_voters,\n",
    "                                                      candidates = candidates)\n",
    "        original_winner, _ = mechanism.calculate(voters = original_election_voters,\n",
    "                                  voter_choices = original_voter_choices)\n",
    "        \n",
    "        other_voters = [voter \n",
    "                        for voter \n",
    "                        in voters \n",
    "                        if not(voter in original_election_voter_IDs)]\n",
    "        \n",
    "\n",
    "        new_voter = random.choice(other_voters)\n",
    "        new_voters = {**original_election_voters,\n",
    "                           new_voter: voters[new_voter]}\n",
    "        new_voter_choices = {**original_voter_choices, \n",
    "                                **{new_voter: {original_winner: 1.0}}}\n",
    "        new_winner, _ = PAWP.calculate(voters = new_voters,\n",
    "                                        voter_choices = new_voter_choices)\n",
    "        if not(new_winner == original_winner):\n",
    "            print(\"We have a vote distortion violation.\")\n",
    "            num_vote_distortions += 1\n",
    "\n",
    "            bad_example = {\"original_voters\": original_election_voters,\n",
    "                            \"original_voter_choices\": voter_choices,\n",
    "                            \"new_voters\": new_voters,\n",
    "                            \"new_voter_choices\": new_voter_choices}\n",
    "            bad_examples.append(bad_example)\n",
    "    \n",
    "        trial_num += 1\n",
    "    \n",
    "    pct_vote_distortion = num_vote_distortions/num_trials\n",
    "\n",
    "    return pct_vote_distortion, bad_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to run the simulation, uncomment it and run below. It takes about 7 minutes to run 100_000 trials on my machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_candidates = [\"A\",\"B\",\"C\",\"D\"]\n",
    "\n",
    "# test_vote_distortion(mechanism = PAWP,\n",
    "#                      voters=voter_weights_dict,\n",
    "#                      candidates = test_candidates,\n",
    "#                      num_trials = 100_000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement #2: Dictatorship and Nakamoto Coefficients\n",
    "\n",
    "Since the mechanism we have chosen simply adds up the weights, we can find certain dictatorial/centralization properties by simply adding up the weights. (What we are calling a \"dictator group\" is sometimes called a *junta* in voting literature, but that is not really relevant to the study.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_smallest_dictator_group(weighted_voters, \n",
    "                                 num_candidates):\n",
    "    total_weight = sum(voter[\"weight\"] \n",
    "                       for voter \n",
    "                       in weighted_voters.values()) #TODO: Change to use helper function. \n",
    "    winning_percentage = 1.0 / num_candidates\n",
    "    winning_weight = total_weight * winning_percentage\n",
    "    sorted_voters = dict(sorted(weighted_voters.items(), \n",
    "                                key=lambda item: item[1][\"weight\"], \n",
    "                                reverse=True))\n",
    "    dictator_group = {}\n",
    "    current_weight = 0\n",
    "    voter_index = 0\n",
    "\n",
    "    while current_weight <= winning_weight:\n",
    "        voter, weight = list(sorted_voters.items())[voter_index]\n",
    "        dictator_group[voter] = weight\n",
    "        current_weight += weight[\"weight\"]\n",
    "        voter_index += 1\n",
    "    return voter_index, dictator_group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Test to See That This Function Works as Intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_voter_sample = {\n",
    "    \"voter_1\": {\"weight\": 10},\n",
    "    \"voter_2\": {\"weight\": 20},\n",
    "    \"voter_3\": {\"weight\": 30},\n",
    "    \"voter_4\": {\"weight\": 40},\n",
    "    \"voter_5\": {\"weight\": 50},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_smallest_dictator_group(weighted_voter_sample,\n",
    "                             num_candidates = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Nakamoto Coefficient Distributions, and Avoiding Dictatorship\n",
    "\n",
    "We look at what the Nakamoto Coefficient can be under different scenarios. \n",
    "\n",
    "### Analysis\n",
    "\n",
    "It is basically impossible that the criterion of being dictator-free can be met under all situations, especially in situations where the voter turnout is low, or only one high-weight individual turns out. It is not hard to construct scenarios for a given turnout, where a dictator would emerge. The question is, how often does this happen? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dictator_groups(weighted_voters, \n",
    "                           min_turnout_pct, \n",
    "                           max_turnout_pct, \n",
    "                           num_trials,\n",
    "                           num_candidates):\n",
    "    smallest_dictator_group_sizes = [0] * num_trials\n",
    "    smallest_dictator_group_makeups = [{}] * num_trials\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        voter_pct = random.uniform(min_turnout_pct, max_turnout_pct)\n",
    "        voter_turnout = generate_random_voter_turnout(weighted_voters, voter_pct)\n",
    "        voter_index, dictator_group = find_smallest_dictator_group(voter_turnout, num_candidates)\n",
    "        \n",
    "        smallest_dictator_group_sizes[i] = len(dictator_group)\n",
    "        smallest_dictator_group_makeups[i] = dictator_group\n",
    "    \n",
    "    return smallest_dictator_group_sizes, smallest_dictator_group_makeups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_TURNOUT_MIN_PCT = 0.08\n",
    "SMALL_TURNOUT_MAX_PCT = 0.12\n",
    "\n",
    "small_turnout_dictator_group_sizes, small_turnout_dictator_group_makeups = analyze_dictator_groups(weighted_voters = voter_weights_dict,\n",
    "                                                                                                   min_turnout_pct = SMALL_TURNOUT_MIN_PCT,\n",
    "                                                                                                   max_turnout_pct = SMALL_TURNOUT_MAX_PCT,\n",
    "                                                                                                   num_trials = 100_000,\n",
    "                                                                                                   num_candidates = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(small_turnout_dictator_group_sizes, bins=10)\n",
    "plt.xlabel('Dictator Group Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Dictator Group Sizes with Small Turnout')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_turnout_dictator_group_weights = [sum_voter_weights(group) for group in small_turnout_dictator_group_makeups]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(small_turnout_dictator_group_weights, bins=10)\n",
    "plt.xlabel('Total Weight of Dictator Group')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Total Weights of Dictator Groups with Small Turnout')\n",
    "plt.show()\n",
    "\n",
    "#TODO: Investigate more? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDIUM_TURNOUT_MIN_PCT = 0.15\n",
    "MEDIUM_TURNOUT_MAX_PCT = 0.20\n",
    "\n",
    "medium_turnout_dictator_group_sizes, medium_turnout_dictator_group_makeups = analyze_dictator_groups(weighted_voters = voter_weights_dict,\n",
    "                                                                                                   min_turnout_pct = MEDIUM_TURNOUT_MIN_PCT,\n",
    "                                                                                                   max_turnout_pct = MEDIUM_TURNOUT_MAX_PCT,\n",
    "                                                                                                   num_trials = 100_000,\n",
    "                                                                                                   num_candidates = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(medium_turnout_dictator_group_sizes, bins=10)\n",
    "plt.xlabel('Dictator Group Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Dictator Group Sizes with Medium Turnout')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_turnout_dictator_group_weights = [sum_voter_weights(group) for group in medium_turnout_dictator_group_makeups]\n",
    "\n",
    "plt.hist(medium_turnout_dictator_group_weights, bins=10)\n",
    "plt.xlabel('Total Weight of Dictator Group')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Total Weights of Dictator Groups with Medium Turnout')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_TURNOUT_MIN_PCT = 0.30\n",
    "LARGE_TURNOUT_MAX_PCT = 0.45\n",
    "\n",
    "large_turnout_dictator_group_sizes, large_turnout_dictator_group_makeups = analyze_dictator_groups(weighted_voters = voter_weights_dict,\n",
    "                                                                                                   min_turnout_pct = LARGE_TURNOUT_MIN_PCT,\n",
    "                                                                                                   max_turnout_pct = LARGE_TURNOUT_MAX_PCT,\n",
    "                                                                                                   num_trials = 100_000,\n",
    "                                                                                                   num_candidates = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(large_turnout_dictator_group_sizes, bins=10)\n",
    "plt.xlabel('Dictator Group Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Dictator Group Sizes with Large Turnout')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_turnout_dictator_group_weights = [sum_voter_weights(group) for group in large_turnout_dictator_group_makeups]\n",
    "\n",
    "plt.hist(large_turnout_dictator_group_weights, bins=10)\n",
    "plt.xlabel('Total Weight of Dictator Group')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Total Weights of Dictator Groups with Large Turnout')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement 3: Sybil Resistance, or the Swifty Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_mint_NFTs = [2,4,6,8,10,21,25,28,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weight = sum(default_weights_dict.get(f\"tokenId {nft}\", 0) for nft in self_mint_NFTs)\n",
    "print(\"Total weight of self-mint NFTs:\", total_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_weights_dict.get(\"tokenId 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_voter_weights(weighted_voters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_swifty_number(voter_weights, established_nft_weight):\n",
    "    total_weight = sum_voter_weights(voter_weights)\n",
    "    newcomers_needed = math.ceil(total_weight / established_nft_weight)  # ceiling division to get the minimum number of newcomers needed\n",
    "    return newcomers_needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_swifty_numbers(weighted_voters, \n",
    "                           min_turnout_pct, \n",
    "                           max_turnout_pct, \n",
    "                           num_trials,\n",
    "                           established_nft_weight):\n",
    "    swifty_numbers = [0] * num_trials\n",
    "    \n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        voter_pct = random.uniform(min_turnout_pct, max_turnout_pct)\n",
    "        voter_turnout = generate_random_voter_turnout(weighted_voters, voter_pct)\n",
    "        current_swifty_number = calculate_swifty_number(voter_turnout, established_nft_weight)\n",
    "        swifty_numbers[i] = current_swifty_number\n",
    "    \n",
    "    return swifty_numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_turnout_swifty_numbers = analyze_swifty_numbers(weighted_voters = voter_weights_dict,\n",
    "                                                      min_turnout_pct = SMALL_TURNOUT_MIN_PCT,\n",
    "                                                      max_turnout_pct = SMALL_TURNOUT_MAX_PCT,\n",
    "                                                      num_trials = 100_000,\n",
    "                                                      established_nft_weight = default_weights_dict.get(\"tokenId 1\"))\n",
    "\n",
    "plt.hist(small_turnout_swifty_numbers, bins=10)\n",
    "plt.xlabel('Swifty Number')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Swifty Numbers with Small Turnout')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_turnout_swifty_numbers = analyze_swifty_numbers(weighted_voters = voter_weights_dict,\n",
    "                                                      min_turnout_pct = MEDIUM_TURNOUT_MIN_PCT,\n",
    "                                                      max_turnout_pct = MEDIUM_TURNOUT_MAX_PCT,\n",
    "                                                      num_trials = 100_000,\n",
    "                                                      established_nft_weight = default_weights_dict.get(\"tokenId 1\"))\n",
    "\n",
    "plt.hist(medium_turnout_swifty_numbers, bins=10)\n",
    "plt.xlabel('Swifty Number')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Swifty Numbers with Medium Turnout')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_turnout_swifty_numbers = analyze_swifty_numbers(weighted_voters = voter_weights_dict,\n",
    "                                                      min_turnout_pct = LARGE_TURNOUT_MIN_PCT,\n",
    "                                                      max_turnout_pct = LARGE_TURNOUT_MAX_PCT,\n",
    "                                                      num_trials = 100_000,\n",
    "                                                      established_nft_weight = default_weights_dict.get(\"tokenId 1\"))\n",
    "\n",
    "plt.hist(large_turnout_swifty_numbers, bins=10)\n",
    "plt.xlabel('Swifty Number')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Swifty Numbers with Large Turnout')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
